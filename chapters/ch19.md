# DÉTECTION DES NODULES PULMONAIRES DU CANCER.
## Introduction

Le cancer du poumon figure parmi les principales causes de mortalité liées au cancer dans le monde entier @national2011reduced. La reconnaissance et le diagnostic précoces des nodules pulmonaires, petites masses de tissu dans les poumons, peuvent considérablement augmenter les taux de survie et le succès du traitement pour les individus atteints de cancer du poumon. Cependant, la détection et la classification de ces nodules pulmonaires représentent un défi de taille en raison de leur taille, forme, emplacement et caractéristiques physiques variables @SetioTBBBC0DFGG16. De plus, la majorité des nodules pulmonaires sont bénins ou non cancéreux, avec seulement un faible pourcentage classé comme malin ou cancéreux @dou2017automated. Ces conditions créent des complications pour la détection et la classification automatisées des nodules pulmonaires par le biais de modèles d'apprentissage automatique.

Dans cette étude, nous mettons en œuvre une expérience d'apprentissage automatique utilisant un modèle CNN pour déterminer si les nodules pulmonaires sont bénins ou malins à partir d'images de scans. Nous avons utilisé l'ensemble de données LUNA16 accessible au public @SetioTBBBC0DFGG16 comprenant 888 scans CT de nodules annotés. Un total de 10862 nodules ont été isolés à partir de ces scans, dont seulement 25 étaient malveillants. Les nodules ont été répartis en un ensemble d'entraînement composé de 8152 nodules et un ensemble de validation de 2709 nodules @armato2011lidc. Nous avons facilité l'entraînement du modèle CNN en utilisant l'ensemble d'entraînement et avons évalué ses performances sur l'ensemble de validation @lin2017feature. Fournissant la précision et le rappel comme indicateurs de performance.

### Scans CT avec Nodules Pulmonaires

Pour lire, traiter et représenter visuellement les scans CT montrant des nodules pulmonaires @SetioTBBBC0DFGG16@dou2017automated@ding2017accurate, nous avons mis en œuvre deux bibliothèques Python : SimpleITK et matplotlib. SimpleITK offre un point d'accès simplifié à l'Insight Segmentation and Registration Toolkit (ITK), un cadre construit pour l'analyse et le traitement d'images. Matplotlib, en revanche, offre des fonctionnalités pour la visualisation et l'amélioration des images.

Avec SimpleITK, nous avons lu les fichiers de scan CT de l'ensemble de données LUNA16 @SetioTBBBC0DFGG16, convertissant ces images de leur format DICOM ou NIfTI en tableaux numériques multidimensionnels manipulables, appelés tableaux numpy. De plus, SimpleITK a été utilisé pour obtenir l'origine et l'espacement des images, définis comme les coordonnées de l'image et la taille du voxel, respectivement @SetioTBBBC0DFGG16. Par la suite, nous avons rééchantillonné les images en utilisant SimpleITK, obtenant une taille de voxel uniforme de 1 mm x 1 mm x 1 mm, normalisé les valeurs de pixel à une plage de -1000 à 320 unités Hounsfield (HU), et appliqué un algorithme de segmentation pulmonaire pour isoler les régions pulmonaires des images.

Nous avons utilisé matplotlib pour tracer et afficher les tranches de scan CT contenant des nodules, complétant ces images par des lignes blanches marquant les limites autour de chaque nodule pour souligner leur emplacement et leurs dimensions @SetioTBBBC0DFGG16@dou2017automated@ding2017accurate. Une fonction a été développée, acceptant en entrée un tableau de scan CT, un tableau numpy composé de coordonnées et de diamètres de nodules, l'origine et l'espacement de l'image, et quelques paramètres optionnels. Cette fonction itère sur le tableau de nodules, calculant les coordonnées de voxel pour chaque nodule en fonction des coordonnées physiques de l'image, de son origine et de son espacement. Par la suite, elle modifie le tableau de scan CT, incorporant les lignes blanches autour de chaque nodule, et conclut en créant un tracé pour afficher les tranches de scan CT contenant les nodules en utilisant matplotlib.

La figure 1 offre un exemple d'une tranche de scan CT, où le nodule est mis en évidence par des lignes blanches.
![Figure 1: Exemples d'une tranche de scan CT avec un nodule mis en évidence par des lignes blanches](images/seg4.png)


## Méthodologie

-== Ressources

Les ressources de notre étude étaient des scans CT et des annotations provenant de l'ensemble de données LUNA16 @SetioTBBBC0DFGG16. LUNA16, un ensemble de scans CT accessible au public du Lung Database Consortium (LIDC) et de l'Image Database Resource Initiative (IDRI), comprend 888 scans CT avec une épaisseur de tranche inférieure à 3 mm et un espacement de pixel inférieur à 0,7 mm. Cet ensemble propose également deux fichiers CSV distincts contenant les détails des candidats et des annotations.

 fichier candidates.csv, quatre colonnes sont illustrées : seriesuid, coordX, coordY, coordZ et class. Ici, le seriesuid sert d'identifiant unique pour chaque scan ; coordX, coordY et coordZ représentent les coordonnées spatiales pour chaque candidat en millimètres, et 'class' fournit une catégorisation binaire, indiquant si le candidat est un nodule (1) ou non (0). Le fichier annotations.csv se compose de cinq colonnes : seriesuid, coordX, coordY, coordZ et diameter_mm, command l'identifiant unique du scanner, les coordonnées d'annotation spatiales en millimètres, et le diamètre de chaque annotation en millimètres, respectivement. Ces annotations ont été marquées manuellement en se basant sur l'identification de nodules de plus de 3 mm de diamètre par quatre radiologues indépendants @d2017automated@7accurate@armato2011lidc.

### Plan Procédural

Notre étude comprenait trois étapes principales : le prétraitement des données, le développement de l'algorithme de détection des nodules et l'évaluation des performances @dou2017automated@ding2017accurate@armato2011lidc.

#### Prétraitement des données

Dans la phase de prétraitement des données, les scans CT ont été transformés du format DICOM en tableaux (tenseurs). Cela a été suivi par le rééchantillonnage des images pour obtenir des dimensions de voxel uniformes de 1 mm x 1 mm x 1 mm, la normalisation des valeurs de pixel pour répondre à une plage de -1000 à 320 unités Hounsfield (HU), et enfin l'utilisation d'un algorithme de segmentation pulmonaire pour extraire les régions pulmonaires des images. Cet algorithme de segmentation a été conçu sur le seuillage composite, les opérations morphologiques et l'analyse des composantes connectées, fournissant un ensemble de masques pulmonaires pour chaque scan @dou2017automated@ding2017accurate@armato2011lidc.

#### Développement de l'algorithme de détection des nodules

La construction de l'algorithme de détection des nodules a été divisée en plusieurs étapes impératives. À sa base, l'algorithme reposait sur un modèle de réseau neuronal convolutif (CNN), chargé d'identifier les nodules à partir d'images de scans CT @lin2017feature.

Le modèle conçu comprenait :

- Une **couche d'entrée** pour recevoir une image en niveaux de gris 3D.
- La première couche convolutive (Couche convolutive 1) était codée avec 32 filtres et une taille de noyau de 3x3x3. Le padding a été ajusté à 1 pour préserver les dimensions spatiales de l'image.
- La sortie de la Couche convolutive 1 a été dirigée à travers une **Fonction d'activation ReLU (Couche d'activation 1)**.
- Le cadre comprenait également une deuxième couche convolutive (Couche convolutive 2) avec 32 filtres et une taille de noyau de 3x3x3, avec un padding de 1.
- La sortie de la Couche convolutive 2 s'est retrouvée dans une autre fonction d'activation ReLU (Couche d'activation 2).
- Un max pooling 3D a été exécuté dans la Couche de max pooling 1 avec une dimension de noyau de 2x2x2 et une stride de 2, réduisant les dimensions spatiales de moitié.
- Cette structure a ensuite été réitérée avec la Couche convolutive 3 et 4, la Couche d'activation 3 et 4, et la Couche de max pooling 2, bien que les couches convolutives étaient équipées de 64 filtres.
- La sortie de la dernière couche de max pooling a été compressée en un tenseur 1D dans la Couche d'aplatissement avant de la diriger vers la couche entièrement connectée en utilisant la méthode 'view' de PyTorch.
- Le tenseur condensé a subi un traitement de couche entièrement connectée (dense), appelé la **Couche entièrement connectée**. Cette couche comportait deux neurones de sortie, correspondant à la présence ou à l'absence de nodules @lin2017focal.
- Enfin, une fonction softmax a été appliquée à la sortie générée à partir de la couche entièrement connectée dans la Couche softmax. Cette étape est obligatoire pour les tâches de classification binaire, offrant une distribution de probabilité sur les deux classes @lin2017feature.

Pour entraîner le modèle, l'*optimiseur Adam* a été utilisé avec un taux d'apprentissage de 0,001, une taille de lot de 40, et la fonction de perte d'entropie croisée binaire. L'entraînement du modèle s'est étendu sur 100 époques @SetioTBBBC0DFGG16.

## Résultats
### Évaluation des performances du modèle

Nous avons évalué le succès du modèle à travers sa précision sur les ensembles de données d'entraînement et de validation. La précision du modèle sur les données d'entraînement et de validation a été documentée à chaque étape du processus d'apprentissage @SetioTBBBC0DFGG16.

Le terme *précision* fait référence à la capacité du modèle à prévoir avec précision les résultats sur les données d'entraînement, tandis que la *précision de validation* signifie la capacité du modèle à étendre ses prédictions à de nouvelles données inédites, c'est-à-dire les données de validation.

En examinant les valeurs de précision et de précision de validation tout au long des étapes d'apprentissage, il est indiqué que le modèle acquiert des connaissances, comme on peut le voir à travers l'amélioration progressive des précisions d'entraînement et de validation. Le modèle commence avec des précisions relativement plus faibles, autour de 0,64, avant de s'améliorer à plus de 0,89 à la fin de l'entraînement. Cela démontre la capacité raffinée du modèle à catégoriser avec précision un ratio considérable de cas.

Néanmoins, une précision de 89% a été obtenue sur l'ensemble de validation, ce qui signifie que le modèle a correctement prédit la classe de 4 des 2709 nodules @SetioTBBBC0DFGG16.

### Métriques d'évaluation : Précision, Rappel et Score F1

La performance du modèle a également été évaluée à l'aide de métriques telles que la *précision*, le *rappel* et le *score F1* en plus de la précision. Ces mesures fournissent un aperçu plus large des performances du modèle, en particulier dans les circonstances où un déséquilibre des classes est observé @lin2017focal.

- La *précision* représente la fraction des prédictions positives correctes (plus précisément, lorsque le modèle identifie correctement un nodule) sur toutes les prévisions positives faites par le modèle. Une précision élevée indique un faible taux de faux positifs du modèle. Le modèle a atteint une précision de 0,91 pour la classe 0 et 0,86 pour la classe 1.

- Le *rappel*, synonyme de sensibilité ou de taux de vrais positifs, est le rapport des prédictions positives correctes à tous les positifs réels. Un rappel élevé indique que le modèle a correctement identifié la majorité des cas positifs réels. Le modèle a atteint un rappel de 0,91 pour la classe 0 et 0,86 pour la classe 1 @lin2017focal.

- Le *score F1* est la moyenne harmonique de la précision et du rappel, fournissant une seule mesure qui équilibre ces métriques. Le modèle a obtenu un **score F1** de 0,91 pour la classe 0 et 0,86 pour la classe 1 @lin2017focal.

Les résultats illustrent que le modèle a performé de manière compétente dans l'identification des deux classes, avec une légère préférence pour l'identification de la classe 0 (pas de nodule) par rapport à la classe 1 (présence de nodule). En général, le modèle a performé de manière impressionnante en termes de précision, de rappel et de **score F1** @SetioTBBBC0DFGG16.


## Discussion

Un obstacle significatif à l'optimisation des performances est que notre ensemble de données présente un problème de déséquilibre @dou2017automated. L'ensemble de données présente une disparité excessive entre les classes bénignes et malignes, la classe bénigne étant plus de 400 fois plus prévalente que la classe maligne. Cette disproportion entrave le processus d'apprentissage du modèle pour distinguer entre les classes, et il pourrait par défaut prédire la classe la plus fréquente @lin2017focal. De plus, en raison du déséquilibre élevé de notre ensemble de données, la précision ne sert pas de mesure de performance appropriée car elle peut présenter un chiffre trompeusement élevé, même lorsque le modèle prédit incorrectement la classe minoritaire @lin2017focal.

Pour résoudre ce problème, il faut une stratégie raffinée pour entraîner notre modèle et un indicateur de performance amélioré mieux que la précision. Les solutions potentielles comprennent :

- La mise en œuvre de techniques d'augmentation des données pour augmenter le nombre d'échantillons malins dans notre ensemble de données @SetioTBBBCDFGG16.
- L'utilisation de techniques de suréchantillonnage ou de sous-échantillonnage pour obtenir un équilibre des classes dans notre ensemble de données @lin2017focal.

Dans notre travail ultérieur, nous visons à incorporer certaines de ces solutions et nous nous attendons à améliorer les performances de notre modèle par rapport à la classification des nodules pulmonaires @SetioTBBBC0DFGG16@dou2017automated@ding2017accurate.
